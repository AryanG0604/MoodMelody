# -*- coding: utf-8 -*-
"""moodmelody_final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mp0E8bZUjry_g_0aMaNyAL-WnnRNquui
"""

from google.colab import drive
import os
import sys
from zipfile import ZipFile

# Mount Google Drive
drive.mount('/content/drive')

# Paths in Google Drive
INPUT_PATH = '/content/drive/MyDrive/moodmelody/kaggle-inputs'
WORKING_PATH = '/content/drive/MyDrive/moodmelody'

# Creating directories if they don't exist
os.makedirs(INPUT_PATH, exist_ok=True)
os.makedirs(WORKING_PATH, exist_ok=True)

# Create symlinks if they don't exist
try:
    os.symlink(INPUT_PATH, os.path.join("..", 'input'), target_is_directory=True)
except FileExistsError:
    pass
try:
    os.symlink(WORKING_PATH, os.path.join("..", 'working'), target_is_directory=True)
except FileExistsError:
    pass

# Function to extract archives
def extract_archive(file_path, extract_path):
    if file_path.endswith('.zip'):
        with ZipFile(file_path, 'r') as zip_ref:
            zip_ref.extractall(extract_path)

# Extract all zip files in the input directory
for root, dirs, files in os.walk(INPUT_PATH):
    for file in files:
        if file.endswith('.zip'):
            file_path = os.path.join(root, file)
            extract_path = os.path.join(INPUT_PATH, os.path.splitext(file)[0])
            if not os.path.exists(extract_path):
                os.makedirs(extract_path)
            print(f"Extracting {file_path} to {extract_path}")
            extract_archive(file_path, extract_path)

print('Data source import complete.')

!pip install spotipy
!pip install streamlit

import re
import nltk
import string
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer

from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split

from tensorflow.keras.utils import to_categorical
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

from tensorflow.keras.optimizers import Adam
from tensorflow.keras.models import Sequential
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.layers import Dense, LSTM, Embedding, Bidirectional

sns.set(font_scale=1.3)
nltk.download("stopwords")
nltk.download('omw-1.4')
nltk.download('wordnet')

import pickle
from imblearn.over_sampling import SMOTE

import spotipy
import numpy as np
import tensorflow as tf
import nltk
import streamlit
import sklearn
import pandas as pd
import psutil
import pkg_resources

spotipy_version = pkg_resources.get_distribution("spotipy").version

print(f"spotipy: {spotipy_version}")
print(f"numpy: {np.__version__}")
print(f"tensorflow: {tf.__version__}")
print(f"nltk: {nltk.__version__}")
print(f"streamlit: {streamlit.__version__}")
print(f"scikit-learn: {sklearn.__version__}")
print(f"pandas: {pd.__version__}")
print(f"psutil: {psutil.__version__}")

# loading datasets
df_train = pd.read_csv('/content/drive/MyDrive/moodmelody/kaggle-inputs/emotions-dataset-for-nlp/train.txt', names=['Text', 'Emotion'], sep=';')
df_val = pd.read_csv('/content/drive/MyDrive/moodmelody/kaggle-inputs/emotions-dataset-for-nlp/val.txt', names=['Text', 'Emotion'], sep=';')
df_test = pd.read_csv('/content/drive/MyDrive/moodmelody/kaggle-inputs/emotions-dataset-for-nlp/test.txt', names=['Text', 'Emotion'], sep=';')

df_train.shape

df_test.shape

df_val.shape

"""asssessing datasets"""

df_train.Emotion.value_counts()

# % of emotions to check if balanced or not
df_train.Emotion.value_counts() / df_train.shape[0] * 100

plt.figure(figsize=(8,4))
sns.countplot(x='Emotion', data=df_train)

# checking for null values
df_train.isnull().sum()

# duplicates
df_train.duplicated().sum()

index = df_train[df_train.duplicated() == True].index
df_train.drop(index, axis=0, inplace=True)
df_train.reset_index(inplace=True, drop=True)

# checking for text duplicates, that is, same text assigned to different emotions
df_train_duplicates = df_train[df_train.Text.duplicated() == True]
df_train_duplicates

# removing duplicated text
index = df_train_duplicates.index
df_train.drop(index, axis=0, inplace=True)
df_train.reset_index(inplace=True, drop=True)

"""repeating the assessment steps for test and validation datasets as well"""

df_test.isnull().sum()

df_test.duplicated().sum()

df_test[df_test.Text.duplicated() == True]

df_val.isnull().sum()

df_val.duplicated().sum()

df_val[df_val.Text.duplicated() == True]

index = df_val[df_val.Text.duplicated() == True].index
df_val.drop(index, axis=0, inplace=True)
df_val.reset_index(inplace=True, drop=True)

# comparing rows of dataframes to check for data leakage
def dataframe_difference(df1, df2, which=None):
    """Finding rows which are different between two DataFrames."""

    comparison_df = df1.merge(
        df2,
        indicator=True,
        how='outer'
    )

    if which is None:
        diff_df = comparison_df[comparison_df['_merge'] != 'both']
    else:
        diff_df = comparison_df[comparison_df['_merge'] == which]

    # Returning the filtered DataFrame
    return diff_df

dataframe_difference(df_train, df_test, which='both')
dataframe_difference(df_train, df_val, which='both')
dataframe_difference(df_val, df_test, which='both')

"""cleaning the data"""

stop_words = set(stopwords.words("english"))
lemmatizer= WordNetLemmatizer()

# preprocessing
def lemmatization(text):
    lemmatizer= WordNetLemmatizer()
    text = text.split()
    text = [lemmatizer.lemmatize(word, pos='v') for word in text]
    return " ".join(text)

def remove_stop_words(text):
    stop_words = set(stopwords.words("english"))
    return " ".join([word for word in text.split() if word not in stop_words])

def remove_numbers(text):
    return ''.join([char for char in text if not char.isdigit()])

def lower_case(text):
    return text.lower()

def remove_punctuations(text):
    text = re.sub(f'[{re.escape(string.punctuation)}]', ' ', text)
    text = re.sub('\s+', ' ', text).strip()
    return text

def remove_urls(text):
    return re.sub(r'https?://\S+|www\.\S+', '', text)

def normalize_text(df):
    df['Text'] = df['Text'].apply(lower_case)
    df['Text'] = df['Text'].apply(remove_stop_words)
    df['Text'] = df['Text'].apply(remove_numbers)
    df['Text'] = df['Text'].apply(remove_punctuations)
    df['Text'] = df['Text'].apply(remove_urls)
    df['Text'] = df['Text'].apply(lemmatization)
    return df

def normalized_sentence(sentence):
    sentence= lower_case(sentence)
    sentence= remove_stop_words(sentence)
    sentence= remove_numbers(sentence)
    sentence= remove_punctuations(sentence)
    sentence= remove_urls(sentence)
    sentence= lemmatization(sentence)
    return sentence

# creating this class just for the sake of pickling
class TextPreprocessor:
    def __init__(self):
        self.lemmatizer = lemmatizer
        self.stop_words = stop_words

    def lemmatization(self, text):
        text = text.split()
        text = [self.lemmatizer.lemmatize(word,pos='v') for word in text]
        return " ".join(text)

    def remove_stop_words(self, text):
        return " ".join([word for word in text.split() if word not in self.stop_words])

    def remove_numbers(self, text):
        return ''.join([char for char in text if not char.isdigit()])

    def lower_case(self, text):
        return text.lower()

    def remove_punctuations(self, text):
        text = re.sub(f'[{re.escape(string.punctuation)}]', ' ', text)
        text = re.sub('\s+', ' ', text).strip()
        return text

    def remove_urls(self, text):
        return re.sub(r'https?://\S+|www\.\S+', '', text)

    def normalized_sentence(self, sentence):
        sentence = self.lower_case(sentence)
        sentence = self.remove_stop_words(sentence)
        sentence = self.remove_numbers(sentence)
        sentence = self.remove_punctuations(sentence)
        sentence = self.remove_urls(sentence)
        sentence = self.lemmatization(sentence)
        return sentence

preprocessor = TextPreprocessor()

with open('/content/drive/MyDrive/moodmelody/text_preprocessor.pkl', 'wb') as file:
    pickle.dump(preprocessor, file)

df_train = normalize_text(df_train)
df_test = normalize_text(df_test)
df_val = normalize_text(df_val)

"""now we will prepare the data for modeling"""

X_train = df_train['Text']
y_train = df_train['Emotion']
X_test = df_test['Text']
y_test = df_test['Emotion']
X_val = df_val['Text']
y_val = df_val['Emotion']

# encoding labels - emotions to encodings
le = LabelEncoder()
y_train = le.fit_transform(y_train)
y_test = le.transform(y_test)
y_val = le.transform(y_val)

print(set(y_train))

# converting these labels to binary class matrix
y_train = to_categorical(y_train)
y_test = to_categorical(y_test)
y_val = to_categorical(y_val)

print(y_train)

emotions = ['anger', 'fear', 'joy', 'love', 'sadness', 'surprise']
le.classes_ = np.array(emotions)
le.classes_

# tokenizing
tokenizer = Tokenizer(oov_token='UNK')
tokenizer.fit_on_texts(pd.concat([X_train, X_test,X_val], axis=0))
vocab_size = len(tokenizer.word_index) + 1

vocab_size

# Convert text to sequences
sequences_train = tokenizer.texts_to_sequences(X_train)
sequences_test = tokenizer.texts_to_sequences(X_test)
sequences_val = tokenizer.texts_to_sequences(X_val)

# Padding sequences to maxlen
maxlen = max([len(seq) for seq in sequences_train])
X_train = pad_sequences(sequences_train, maxlen=maxlen, truncating='pre')
X_test = pad_sequences(sequences_test, maxlen=maxlen, truncating='pre')
X_val = pad_sequences(sequences_val, maxlen=maxlen, truncating='pre')

# before padding
sequences_train[0]

# after padding
X_train[0]

# pickling the tokenizer, label encoder and maxlen for future use
with open('/content/drive/MyDrive/moodmelody/tokenizer.pkl', 'wb') as file:
  pickle.dump(tokenizer, file)

with open('/content/drive/MyDrive/moodmelody/label_encoder.pkl', 'wb') as file:
  pickle.dump(le, file)

with open('/content/drive/MyDrive/moodmelody/maxlen.pkl', 'wb') as file:
  pickle.dump(maxlen, file)

# Loading GloVe embeddings
path_to_glove_file = '/content/drive/MyDrive/moodmelody/kaggle-inputs/glove.6B.200d.txt/glove.6B.200d.txt'
embedding_dim = 200
embeddings_index = {}

# reading word vectors
with open(path_to_glove_file) as f:
    for line in f:
        word, *vector = line.split()
        embeddings_index[word] = np.array(vector, dtype=np.float32)

len(embeddings_index)

# Creating embedding matrix(assigning word vectors to our vocab)
embedding_matrix = np.zeros((vocab_size, embedding_dim))
hits, misses = 0, 0
for word, i in tokenizer.word_index.items():
    embedding_vector = embeddings_index.get(word)
    if embedding_vector is not None:
        embedding_matrix[i] = embedding_vector
        hits += 1
    else:
        misses += 1

print(f"Converted {hits} words ({misses} misses)")

"""building the model"""

print("Embedding matrix shape:", embedding_matrix.shape)

from tensorflow.keras.layers import BatchNormalization, Dropout
# defining and compiling
model = Sequential([
    Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], trainable=False),
    Bidirectional(LSTM(64, return_sequences=True)),
    Bidirectional(LSTM(64)),
    Dense(64, activation='relu'),
    Dense(6, activation='softmax')
])

model.compile(optimizer=Adam(learning_rate=0.0003), loss='categorical_crossentropy', metrics=['accuracy'])
model.build(input_shape=(None, maxlen))
model.summary()

# Training model with early stopping
from keras.callbacks import ReduceLROnPlateau
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=0.0001)
history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=20, batch_size=64, callbacks=[early_stopping])

# Evaluating model
loss, accuracy = model.evaluate(X_test, y_test)
print(f'Test Loss: {loss:.4f}')
print(f'Test Accuracy: {accuracy * 100:.2f}%')

print(history.history.keys())

# visualising loss and accuracy
train_loss = history.history['loss']
train_accuracy = history.history['accuracy']

val_loss = history.history['val_loss']
val_accuracy = history.history['val_accuracy']

import matplotlib.pyplot as plt

plt.plot(range(1, len(train_loss) + 1), train_loss, label='Training Loss')
plt.plot(range(1, len(val_loss) + 1), val_loss, label='Validation Loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

# Plotting accuracy
plt.plot(range(1, len(train_accuracy) + 1), train_accuracy, label='Training Accuracy')
plt.plot(range(1, len(val_accuracy) + 1), val_accuracy, label='Validation Accuracy')
plt.title('Training and Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

from sklearn.metrics import classification_report
y_pred = model.predict(X_test)
y_pred_labels = np.argmax(y_pred, axis=1)
y_true_labels = np.argmax(y_test, axis=1)
print(classification_report(y_true_labels, y_pred_labels))

# some examples
sentences = [
           "He's over the moon about being accepted to the university",
           "Your point on this certain matter made me outrageous, how can you say so? This is insane.",
           "I can't do it, I'm not ready to lose anything, just leave me alone",
           "Merlin's beard harry, you can cast the Patronus charm! I'm amazed!",
          "I can't believe they did that to me; it's so frustrating and unfair!",
          "Walking alone at night in that neighborhood gives me chills and makes me anxious.",
          "Winning the competition filled me with immense happiness and excitement.",
          "Every time I see you, my heart fills with warmth and affection.",
           "The news of his departure brought tears to my eyes and a deep sense of sorrow.",
           "I was completely taken aback when they threw me a surprise birthday party!"


            ]
for sentence in sentences:
    print(sentence)
    sentence = normalized_sentence(sentence)
    sentence = tokenizer.texts_to_sequences([sentence])
    sentence = pad_sequences(sentence, maxlen=maxlen, truncating='pre')

    predictions = model.predict(sentence)
    predicted_label = np.argmax(predictions, axis=-1)[0]
    print(f"Predicted Label: {predicted_label}")  # Debug print to check the predicted label
    predicted_emotion = le.inverse_transform([predicted_label])[0]  # Convert label back to emotion

    proba =  np.max(predictions)
    print(f"{predicted_emotion} : {proba}\n\n")

import numpy as np
from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

y_pred_prob = model.predict(X_test)
y_pred_class = np.argmax(y_pred_prob, axis=1)
y_true = np.argmax(y_test, axis=1)

cm = confusion_matrix(y_true, y_pred_class)
print(cm)

emotion_labels = ['anger', 'fear', 'joy', 'love', 'sadness', 'surprise']


plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=emotion_labels, yticklabels=emotion_labels)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()

model.save('/content/drive/MyDrive/moodmelody/Emotion Recognition from text 3.h5')

!pip install spotipy

pip install streamlit

# Commented out IPython magic to ensure Python compatibility.
# # app.py
# %%writefile app.py
# 
# import streamlit as st
# import spotipy
# from spotipy.oauth2 import SpotifyOAuth
# import numpy as np
# import pickle
# import tensorflow as tf
# from tensorflow.keras.preprocessing.sequence import pad_sequences
# import re
# import string
# from nltk.corpus import stopwords
# from nltk.stem import WordNetLemmatizer
# import logging
# 
# logging.basicConfig(level=logging.DEBUG)
# 
# stop_words = set(stopwords.words("english"))
# lemmatizer = WordNetLemmatizer()
# 
# # Replace with your Spotify app credentials
# CLIENT_ID = 'dc88dca2f2e549bbbdef653f12ddb042'
# CLIENT_SECRET = '00e8ea8136994ceaada7b3833c88edcf'
# REDIRECT_URI = 'http://localhost:5000/callback'
# 
# # Authentication
# sp = spotipy.Spotify(auth_manager=SpotifyOAuth(client_id=CLIENT_ID,
#                                                client_secret=CLIENT_SECRET,
#                                                redirect_uri=REDIRECT_URI,
#                                                scope="user-modify-playback-state user-read-playback-state",
#                                                open_browser=False))
# 
# sp_oauth = SpotifyOAuth(client_id=CLIENT_ID,
#                         client_secret=CLIENT_SECRET,
#                         redirect_uri=REDIRECT_URI,
#                         scope="user-modify-playback-state user-read-playback-state",
#                         cache_path="./.spotifycache")
# 
# # Define song URIs for each emotion
# emotion_to_song_uri = {
#     'anger': 'spotify:track:7iN1s7xHE4ifF5povM6A48',    # let it be
#     'fear': 'spotify:track:3KkXRkHbMCARz0aVfEt68P',     # sunflower
#     'joy': 'spotify:track:7qiZfU4dY1lWllzX7mPBI3',      # shape of you
#     'love': 'spotify:track:3d9DChrdc6BOeFsbrZ3Is0',     # under the bridge
#     'sadness': 'spotify:track:008McaJl3WM1UqxxVie9BP',  # the wisp sings
#     'surprise': 'spotify:track:10nyNJ6zNy2YVYLrcwLccB'  # no surprises
# }
# 
# # Load the saved model, tokenizer, label encoder, and maxlen
# model_path = '/content/drive/MyDrive/moodmelody/Emotion Recognition from text 3.h5'
# tokenizer_path = '/content/drive/MyDrive/moodmelody/tokenizer.pkl'
# label_encoder_path = '/content/drive/MyDrive/moodmelody/label_encoder.pkl'
# maxlen_path = '/content/drive/MyDrive/moodmelody/maxlen.pkl'
# 
# try:
#     model = tf.keras.models.load_model(model_path)
#     with open(tokenizer_path, 'rb') as file:
#         tokenizer = pickle.load(file)
#     with open(label_encoder_path, 'rb') as file:
#         le = pickle.load(file)
#     with open(maxlen_path, 'rb') as file:
#         maxlen = pickle.load(file)
#     st.success('Model and tokenizer loaded successfully!')
# except OSError as e:
#     st.error(f'Error loading model: {e}')
# 
# # Preprocessing steps
# def lemmatization(text):
#     text = text.split()
#     text = [lemmatizer.lemmatize(word) for word in text]
#     return " ".join(text)
# 
# def remove_stop_words(text):
#     return " ".join([word for word in text.split() if word not in stop_words])
# 
# def remove_numbers(text):
#     return ''.join([char for char in text if not char.isdigit()])
# 
# def lower_case(text):
#     return text.lower()
# 
# def remove_punctuations(text):
#     text = re.sub(f'[{re.escape(string.punctuation)}]', ' ', text)
#     text = re.sub('\s+', ' ', text).strip()
#     return text
# 
# def remove_urls(text):
#     return re.sub(r'https?://\S+|www\.\S+', '', text)
# 
# def normalized_sentence(sentence):
#     sentence = lower_case(sentence)
#     sentence = remove_stop_words(sentence)
#     sentence = remove_numbers(sentence)
#     sentence = remove_punctuations(sentence)
#     sentence = remove_urls(sentence)
#     sentence = lemmatization(sentence)
#     return sentence
# 
# # Function to preprocess and predict emotion
# def predict_emotion(text):
#     try:
#         # Preprocessing
#         text = normalized_sentence(text)
#         sequence = tokenizer.texts_to_sequences([text])
#         padded_sequence = pad_sequences(sequence, maxlen=maxlen, truncating='pre')
# 
#         # Predict emotion
#         prediction = model.predict(padded_sequence)
#         predicted_label = np.argmax(prediction, axis=1)
#         predicted_emotion = le.inverse_transform(predicted_label)[0]
# 
#         return predicted_emotion
# 
#     except Exception as e:
#         st.error(f"An error occurred during prediction: {e}")
#         return None
# 
# # Function to play song based on emotion
# def play_song(emotion):
#     song_uri = emotion_to_song_uri.get(emotion)
#     if song_uri:
#         try:
#             devices = sp.devices()
#             if devices['devices']:
#                 device_id = devices['devices'][0]['id']
#                 sp.start_playback(device_id=device_id, uris=[song_uri])
#                 st.write(f"Playing song for {emotion}: {song_uri}")
#             else:
#                 st.write("No active devices found.")
# 
#         except spotipy.SpotifyException as e:
#             st.error(f"An error occurred while playing song: {e}")
#     else:
#         st.write("No song found for this emotion.")
# 
# def main():
#     st.title("MoodMelody: Emotion-based Music Recommender")
# 
#     text = st.text_input("Enter how you are feeling:")
#     if st.button("Detect Emotion and Play Song"):
#         detected_emotion = predict_emotion(text)
#         if detected_emotion:
#             st.write(f"Detected emotion: {detected_emotion}")
# 
#             # Display the authentication URL if token is not cached
#             token_info = sp_oauth.get_cached_token()
#             if not token_info:
#                 auth_url = sp_oauth.get_authorize_url()
#                 st.markdown(f"[Authenticate with Spotify]({auth_url})")
#                 code = st.text_input("Enter the code from the URL after authentication:")
#                 if code:
#                     try:
#                         token_info = sp_oauth.get_access_token(code)
#                         global sp
#                         sp = spotipy.Spotify(auth=token_info['access_token'])
#                         play_song(detected_emotion)
#                     except spotipy.SpotifyOauthError as e:
#                         st.error(f"Spotify OAuth error: {e}")
#             else:
#                 sp = spotipy.Spotify(auth=token_info['access_token'])
#                 play_song(detected_emotion)
# 
# if __name__ == "__main__":
#     main()

!curl icanhazip.com

!streamlit run app.py & npx localtunnel --port 8501

